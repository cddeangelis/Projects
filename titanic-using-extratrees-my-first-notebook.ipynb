{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nHello y'all, this is notebook descibes my approach to the [Titanic Competition](https://www.kaggle.com/competitions/titanic/), a competition for beginners on Kaggle.\n\nI am a beginner! This is my first notebook so please share feedback, better ways to of doing things, or anything that could be helpful! I am getting started with all of this and I am here to learn.","metadata":{}},{"cell_type":"markdown","source":"# The Titanic Dataset\n\nThe Titanic is the famous ship that hit an iceberg. The first thing I did for this was skim through the [wikipedia page](https://en.wikipedia.org/wiki/Titanic), which I'd recommned for anyone to refresh the story. I was mainly looking for information about survivors and how they might relate to our features. The important line I read was the lifeboats were filled with \"women and children first.\" So right off the bat, sex and age are going to be important. \n\nThe training set contains 10 columns:\n\n    survival: 0 = No, 1 = Yes\n    pclass: Ticket class: 1 = 1st, 2 = 2nd, 3 = 3rd\n    sex: Sex\n    Age: Age in years\n    sibsp: # of siblings / spouses aboard the Titanic\n    parch: # of parents / children aboard the Titanic\n    ticket: Ticket number\n    fare: Passenger fare\n    cabin: Cabin number:\n    embarked: Port of Embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\n### First off, installing libraries and a look at the data:\n    ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ndata = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:25:56.851837Z","iopub.execute_input":"2022-10-09T02:25:56.852338Z","iopub.status.idle":"2022-10-09T02:25:56.886458Z","shell.execute_reply.started":"2022-10-09T02:25:56.852293Z","shell.execute_reply":"2022-10-09T02:25:56.885004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another look:","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-10-09T02:26:00.137376Z","iopub.execute_input":"2022-10-09T02:26:00.137969Z","iopub.status.idle":"2022-10-09T02:26:00.185424Z","shell.execute_reply.started":"2022-10-09T02:26:00.137915Z","shell.execute_reply":"2022-10-09T02:26:00.183732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right away, we can see a couple important things about the data:\n\n1. Age and Cabin are missing values\n\n2. PassengerID, Name, Ticket, and Cabin aren't going to be very useful for our model<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1)\n3. Sex is binary (duh)\n4. Age and Fare have pretty big ranges\n\n\nBut before and data cleaning, let's see if we can find any trends.\n\n# Data Exploration","metadata":{}},{"cell_type":"markdown","source":"We already know women were prioritized in the lifeboats, but let's look at sex:","metadata":{}},{"cell_type":"code","source":"men = data.loc[data.Sex == 'male'][\"Survived\"]\nwomen = data.loc[data.Sex == 'female']['Survived']\n\nprint(\"Men:   \" + str(round(100*sum(men)/len(men))) + \"% survival\")\nprint(\"Women: \" + str(round(100*sum(women)/len(women))) + \"% survival\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:26:03.305090Z","iopub.execute_input":"2022-10-09T02:26:03.305578Z","iopub.status.idle":"2022-10-09T02:26:03.318742Z","shell.execute_reply.started":"2022-10-09T02:26:03.305539Z","shell.execute_reply":"2022-10-09T02:26:03.317196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a large discrepency, so this is going to be an important feature. \n\nNext let's take a look at Passenger Class: ","metadata":{}},{"cell_type":"code","source":"first = data.loc[data.Pclass == 1][\"Survived\"]\nsecond = data.loc[data.Pclass == 2][\"Survived\"]\nthird = data.loc[data.Pclass == 3][\"Survived\"]\n\nprint(\"survival, by passenger class:\\n\\n\"\n      +\"First: \"+str(round(100*sum(first)/(len(first))))+\"%\\n\"\n      +\"Second: \" + str(round(100*sum(second)/(len(second))))+\"%\\n\"\n      +\"Third: \"+str(round(100*sum(third)/(len(third)))) + \"%\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T03:21:40.129550Z","iopub.execute_input":"2022-10-09T03:21:40.130116Z","iopub.status.idle":"2022-10-09T03:21:40.145321Z","shell.execute_reply.started":"2022-10-09T03:21:40.130055Z","shell.execute_reply":"2022-10-09T03:21:40.143890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not looking too good for third class. I guess the wealthy always come out on top. \n\nNext up: Age","metadata":{}},{"cell_type":"code","source":"ageS = data.loc[data.Survived == 1][\"Age\"]\nageD = data.loc[data.Survived == 0][\"Age\"]\n\nplt.hist(ageS, alpha=0.5, bins=15, label='Survived')\nplt.hist(ageD, alpha=0.5, bins=15, label='Died')\nplt.legend(loc='upper right')\nplt.title(\"Age distribution by survivial\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-09T02:26:08.322137Z","iopub.execute_input":"2022-10-09T02:26:08.322635Z","iopub.status.idle":"2022-10-09T02:26:08.604093Z","shell.execute_reply.started":"2022-10-09T02:26:08.322597Z","shell.execute_reply":"2022-10-09T02:26:08.602752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is interesting! Children are the only age group that were more likely to survive, but people in their 30s did a lot better than those 18-25. I'm guessing this is because of parents? If families were prioritized, which I think they were, this would make sense. \n\nI wonder what this would look like split up by sex?","metadata":{}},{"cell_type":"code","source":"ageSmen = data.loc[(data.Survived == 1) & (data.Sex == 'male')][\"Age\"]\nageDmen = data.loc[(data.Survived == 0) & (data.Sex == 'male')][\"Age\"]\n\nageSwomen = data.loc[(data.Survived == 1) & (data.Sex == 'female')][\"Age\"]\nageDwomen = data.loc[(data.Survived == 0) & (data.Sex == 'female')][\"Age\"]\n\nplt.hist(ageSmen, alpha=0.5, bins=15, label='Male Survived')\nplt.hist(ageDmen, alpha=0.5, bins=15, label='Male Died')\nplt.hist(ageSwomen, alpha=0.5, bins=15, label='Female Survived')\nplt.hist(ageDwomen, alpha=0.5, bins=15, label='Female Died')\nplt.legend(loc='upper right')\nplt.title(\"Age distribution by survivial\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-09T02:26:11.776998Z","iopub.execute_input":"2022-10-09T02:26:11.778327Z","iopub.status.idle":"2022-10-09T02:26:12.115040Z","shell.execute_reply.started":"2022-10-09T02:26:11.778267Z","shell.execute_reply":"2022-10-09T02:26:12.113739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gist of the story: male only survived if they were children or fathers (I'm just guessing)\n\n\nFinally let's look at Fare:","metadata":{}},{"cell_type":"code","source":"fareS = data.loc[(data.Survived == 1)][\"Fare\"]\nfareD = data.loc[(data.Survived == 0)][\"Fare\"]\n\nplt.figure(2)\nplt.hist(fareS, alpha=0.5, bins=15, label='Survived')\nplt.hist(fareD, alpha=0.5, bins=15, label='Died')\n\nplt.legend(loc='upper right')\nplt.title(\"Fare distribution by survivial\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-10-09T02:26:23.255426Z","iopub.execute_input":"2022-10-09T02:26:23.255969Z","iopub.status.idle":"2022-10-09T02:26:23.474453Z","shell.execute_reply.started":"2022-10-09T02:26:23.255924Z","shell.execute_reply":"2022-10-09T02:26:23.472445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This isn't that helpful, but reinforces the idea that the people who paid more had a higher chance of survival. I'm guessing fare would correlate pretty strongly with passenger class","metadata":{}},{"cell_type":"markdown","source":"# Feature Selection\n\nI decided to use Pclass, Sex, Age, and Fare. Presumably it's more helpful to talk about the features I *didn't* use. \n\nboth SibSp and Parch  were slighly correlated with survival, but they weren't useful in my experimenting. I saw someone do some feature engineering and create a FamilySize feature, which they did by combining SibSp, Parch, and last names. This is cool, but outside of my current scope. It might be interesting to play around with different ways of combining these features to improve the model.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Cleaning and Engineering\n\nSo we have our features, we can just put them into a model right??\n\nNope. I did this on the first run and the model is no better than random. The data needs to be cleaned. NaNs need to be filled, 'male' and 'female' needs to be encoded, and bins need to be created for age and fare. \n\nBinning is a helpful technique for transforming a continuous variable into categorical data. With ordinal binning, the order can be maintained. It reduces the chances of overfitting and often improves models.\n\nEncoding is assigning numerical values to categorical data. Binary encoding is assigning 'male' as 0 and 'female' as 1. Ordinal encoding is assigning ordered categorical data, such as the bins we create for age. One-hot coding is for encoding nominal data, a column is created for each category and 0s and 1s are input accordingly. This would be helpful if \"Embarked\" was used as a feature. \n\nFor age, the most important bin seemed to be children, so I decided to create the bins every 10 years, i.e. 0-10, 10-20, etc. I stopped at 60+, which was arbitrary. \n\nFor fares, I used 8 equal frequency bins. Given the distribution of fares, I think frequency based binning is apporpriate. 8 is an arbitrary number. \n\nThe age and fare bins are then encoded as 0,1,2,3,etc. \n\nAge NaNs are filled with the mean age. \n\nFinally I  drop columns that aren't going to be used. \n","metadata":{}},{"cell_type":"code","source":"def preprocessing(data):\n    \n    # Fill NaNs with mean\n    meanAge = data.Age.mean()\n    data.Age = data.Age.fillna(meanAge)\n\n    # create and encode Age bins\n    Abins = [0,10,20,30,40,50,60]\n    Abin_Labels = [0,1,2,3,4,5]\n    data['Age_bins'] = pd.cut(data['Age'], Abins, labels=Abin_Labels)\n\n    # create and encode Fare bins\n    Fbin_labels = [0,1,2,3,4,5,6,7]\n    data['Fare_bins'] = pd.qcut(data['Fare'], q=8, labels=Fbin_labels) # 8 is arbitrary, unsure what ideal would be\n\n    # encode Sex\n    data.Sex = data.Sex.replace({\"male\":0, \"female\":1})\n\n    # drop unneccessary columns\n    data = data.drop(['PassengerId', 'Name', 'Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked'],axis=1)\n    if \"Survived\" in data.columns:\n        data = data.drop(['Survived'], axis=1)\n        \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:26:28.511273Z","iopub.execute_input":"2022-10-09T02:26:28.512353Z","iopub.status.idle":"2022-10-09T02:26:28.522713Z","shell.execute_reply.started":"2022-10-09T02:26:28.512300Z","shell.execute_reply":"2022-10-09T02:26:28.521303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next apply the preprocessing function to the data. grab the truth values of train_data and passenger IDs of the test_data before they're dropped in the preprocessing","metadata":{}},{"cell_type":"code","source":"y = data.iloc[:,1]\npID = test_data.PassengerId\n\ntrain_data = preprocessing(data)\ntest_data = preprocessing(test_data)\n\nX = train_data","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:26:32.454514Z","iopub.execute_input":"2022-10-09T02:26:32.455436Z","iopub.status.idle":"2022-10-09T02:26:32.480186Z","shell.execute_reply.started":"2022-10-09T02:26:32.455388Z","shell.execute_reply":"2022-10-09T02:26:32.478734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model\n\nFinally get to create the model. I chose Extra Trees over Random Forest because it is less prone to bias. Of course it's more prone to variance, but I thought less bias was best for this data. I tried out a bunch of different models, as well as various combinations of the best ones. Extra Trees tied for the best performance I got. A combination of Gradient Boost and Ada Boost performed just as well. Throwing in KNN got about the same results. Given I couldn't find any combination or single classifier that was better than Extra Trees, I went with that ","metadata":{}},{"cell_type":"code","source":"model = ExtraTreesClassifier()\nmodel.fit(X,y)\npredictions = model.predict(test_data)\n\noutput = pd.DataFrame({'PassengerId': pID, 'Survived': predictions})\n\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-10-09T02:26:36.034856Z","iopub.execute_input":"2022-10-09T02:26:36.035330Z","iopub.status.idle":"2022-10-09T02:26:36.217085Z","shell.execute_reply.started":"2022-10-09T02:26:36.035286Z","shell.execute_reply":"2022-10-09T02:26:36.215725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThis model scores a 0.77272, which is solid. Looking at the leaderboards, this nearly the best possible score, without using the extended data or creating a FamilySize feature. So I am happy with it. Like I mentioned at the beginning, **please give any feedback!**\n\n### Ways to Improve\n \nThere are a bunch of specific hyperparameters that people have figured out that will improve the model. I haven't really looked at what these specifically do.\n\nScaling the data? might help, not sure\n\nTrying different bin sizes and distributions. Using 6 and 8 bins was arbitrary, this is likely could be improved. \n\nFeature Engineering: look up the ways people create a FamilySize feature and some other things, these can improve the model.\n\nExtended data set: can be found online, imo this is cheating for this challenge though\n\n","metadata":{}},{"cell_type":"markdown","source":"<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) people have found ways to use these but I'm not going to","metadata":{}}]}