{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-17T21:49:45.267949Z","iopub.execute_input":"2022-12-17T21:49:45.268746Z","iopub.status.idle":"2022-12-17T21:49:45.275326Z","shell.execute_reply.started":"2022-12-17T21:49:45.268708Z","shell.execute_reply":"2022-12-17T21:49:45.274510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hello!\n\nI am using a [dataset](https://www.kaggle.com/datasets/whenamancodes/students-performance-in-exams) of 1000 students to build a model that predicts test scores. We'll get some insight into what factors affect student performance. First I'll visualize at the data to see how scores vary between different features, then optimize a few models to make predictions.\n\nI am using this as an exercise to practice hyperparameter optimization. I use Grid Search with 5-fold cross-validation to find the optimal hyperparameters for Random Forest and XGBoost. \n\nThis dataset is relatively small and not ideal for this purpose, but this was good to do nonetheless.","metadata":{}},{"cell_type":"markdown","source":"# Setup\n\nQuick look at the data. We have five features and three scores per sample. To make things simpler I create a 'score' column, which is the average of the math, reading, and writing scores. \n\nAside from the scores, it is all categorical data. Thankfully it is clean and should be easy to encode. This dataset has no NaNs.  Gender, lunch, and test preparation are all binary. Parental level of education is the only ordered feature, so I'll use ordinal encoding. Finally race/ethnicity is unordered, so I'll use one-hot encoding for that.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/students-performance-in-exams/exams.csv')\n\nX_ob = [col for col in data if data[col].dtype == 'object']\nX_ob = pd.DataFrame(data[X_ob])\n\ny_values = data.drop(X_ob, axis=1)\n\nn = X_ob['race/ethnicity']\nn.name = 'new'\nX_ob = pd.concat([X_ob, n], axis=1)\n\ny = y_values.mean(axis=1)\ny.name = 'score'\n\ndata['score'] = y\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T21:49:50.371802Z","iopub.execute_input":"2022-12-17T21:49:50.372217Z","iopub.status.idle":"2022-12-17T21:49:50.408906Z","shell.execute_reply.started":"2022-12-17T21:49:50.372180Z","shell.execute_reply":"2022-12-17T21:49:50.407713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\n\nLet's take at the data. We have a few groups where we'd expect score disparities.\n\nParental level of education, standard vs free lunch, and test preparation probably all affect scores about how you would expect\n\nLet's take a look:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nSHS = data.loc[data['parental level of education'] == 'some high school']['score']\nHS = data.loc[data['parental level of education'] == 'high school']['score']\nSC = data.loc[data['parental level of education'] == 'some college']['score']\nAD = data.loc[data['parental level of education'] == \"associate's degree\"]['score']\nBD = data.loc[data['parental level of education'] == \"bachelor's degree\"]['score']\nMD = data.loc[data['parental level of education'] == \"master's degree\"]['score']\n\neducationData = [SHS, HS, SC, AD, BD, MD]\n\nfig, ax = plt.subplots(figsize=(10,6))\nax.boxplot(educationData, vert=True)\nax.set_ylabel(\"Score\")\n\nax.set_xticklabels(['some high school', 'high school','some college', \n              \"associate's degree\", \"bachelor's degree\", \"master's degree\"])\nax.set_title('Test scores between Parental levels of education')\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-17T22:18:10.103247Z","iopub.execute_input":"2022-12-17T22:18:10.103655Z","iopub.status.idle":"2022-12-17T22:18:10.380338Z","shell.execute_reply.started":"2022-12-17T22:18:10.103620Z","shell.execute_reply":"2022-12-17T22:18:10.379199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"StandardLunch = data.loc[data['lunch'] == 'standard']['score']\nFreeLunch = data.loc[data['lunch'] == 'free/reduced']['score']\nLunchdata = [StandardLunch, FreeLunch]\n\nprep = data.loc[data['test preparation course'] == 'completed']['score']\nnoprep = data.loc[data['test preparation course'] == 'none']['score']\nprepdata = [prep, noprep]\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\nax1.boxplot(Lunchdata, vert=True)\nax1.set_xticklabels(['Standard', 'Free/Reduced'])\nax1.set_title('Test scores between Standard/Free Lunch')\nax1.set_ylabel(\"Score\")\n\nax2.boxplot(prepdata, vert=True)\nax2.set_xticklabels(['completed', 'none'])\n\nax2.set_title('Test scores between prep/no prep')\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-17T22:18:35.053570Z","iopub.execute_input":"2022-12-17T22:18:35.053963Z","iopub.status.idle":"2022-12-17T22:18:35.373385Z","shell.execute_reply.started":"2022-12-17T22:18:35.053931Z","shell.execute_reply":"2022-12-17T22:18:35.372575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're given five race/ethnic groups, let's take a look at those","metadata":{}},{"cell_type":"code","source":"gA = data.loc[data['race/ethnicity'] == 'group A']['score']\ngB = data.loc[data['race/ethnicity'] == 'group B']['score']\ngC = data.loc[data['race/ethnicity'] == 'group C']['score']\ngD = data.loc[data['race/ethnicity'] == 'group D']['score']\ngE = data.loc[data['race/ethnicity'] == 'group E']['score']\n\ndataM = [gA, gB, gC, gD, gE]\n\nfig, ax = plt.subplots()\nax.boxplot(dataM, vert=True)\nax.set_xticklabels(['group A', 'group B', 'group C', 'group D', 'group E'])\nax.set_title('Test scores between ethnicities')\nax.set_ylabel(\"Score\")\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-12-17T22:18:43.089894Z","iopub.execute_input":"2022-12-17T22:18:43.090297Z","iopub.status.idle":"2022-12-17T22:18:43.328366Z","shell.execute_reply.started":"2022-12-17T22:18:43.090264Z","shell.execute_reply":"2022-12-17T22:18:43.327023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding\n\nI'm creating a preprocessor to encode the data. As mentioned earlier, I'll use one-hot encoding for race/ethincity, ordinal encoding for parental level of education, and binary encoding for the remaining features. ","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\neducation_categories = [['some high school', 'high school','some college', \n              \"associate's degree\", \"bachelor's degree\", \"master's degree\"]]\n\noh_cols = ['race/ethnicity', 'new']\nord_cols = [col for col in X_ob if col not in oh_cols and col != 'parental level of education']\n\n\n# Encode the columsn how we want, Ordinal for education, one-hot for race, and binary for the rest\nedu_transformer = OrdinalEncoder(categories=education_categories)\nord_transformer = OrdinalEncoder()\noh_transformer = OneHotEncoder()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('edu',edu_transformer, ['parental level of education']),\n        ('ord',ord_transformer, ord_cols),\n        ('oh',oh_transformer, oh_cols)\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-12-17T21:50:07.674149Z","iopub.execute_input":"2022-12-17T21:50:07.674570Z","iopub.status.idle":"2022-12-17T21:50:08.125968Z","shell.execute_reply.started":"2022-12-17T21:50:07.674534Z","shell.execute_reply":"2022-12-17T21:50:08.124988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning with GridSearchCV\n\nHyperparameters of an algorithm can be thought of as settings that affect the model's performance. *Parameters* are values that are learned during training, such as the slope and intercept in linear regression, or weights in a neural network. Hyperparameters, on the other hand, are set beforehand and are not changed during training. The process of optimizing hyperparameters is called hyperparameter tuning.\n\nTwo common methods of hyperparameter optimization are Grid Search and Random Search. Grid search exhaustively searches the hyperparameter space to find the optimal settings, which can be computationally expensive. Random search can be used when the hyperparameter space is large and will typically return good results. Here I will be using GridSearchCV, which incorporates cross-validation and exhaustively searches the hyperparameter combinations.\n\nI am defining a funcition whose arguments are the model and the paramter grid to be searched. I do this to easily incorporate the preprocessing and model in a pipeline, which is passed into GridSearchCV. The mean abolsolute error is used as scoring with 5-fold cross-validation. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\ndef get_hyper(model, p_grid):\n\n    # Combine established preprocessor and argument model into a pipeline\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('clf', model)\n                     ])\n\n    gs = GridSearchCV(estimator=clf, param_grid=p_grid, cv=5,scoring='neg_mean_absolute_error')\n    gs.fit(X_ob, y)\n\n    \n    return gs","metadata":{"execution":{"iopub.status.busy":"2022-12-17T21:50:12.074294Z","iopub.execute_input":"2022-12-17T21:50:12.075347Z","iopub.status.idle":"2022-12-17T21:50:12.097841Z","shell.execute_reply.started":"2022-12-17T21:50:12.075297Z","shell.execute_reply":"2022-12-17T21:50:12.096401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n\nTime to evaluate XGBoost, a gradient boosting model that is quite popular. You can read more about it [here](https://www.kaggle.com/code/alexisbcook/xgboost).\n\nI am searching for the optimal n_estimators and learning_rate. \n\nHere MAE across the hyperparameter space:","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(random_state=0)\np_grid = {'clf__n_estimators': [50, 60, 70, 80, 90, 100, 110, 120, 130],\n              'clf__learning_rate':[0.01, 0.03, 0.05, 0.07, 0.09, 0.11]}\n\ngs = get_hyper(model, p_grid)\n\nprint(\"Best MAE: %f using %s\" % (gs.best_score_, gs.best_params_))\n\nmeans = gs.cv_results_['mean_test_score']\nscores = np.array(means).reshape(len(p_grid['clf__learning_rate']), len(p_grid['clf__n_estimators']))\n\nfig, ax = plt.subplots()\nfor i, value in enumerate([\"0.03\", \"0.05\", \"0.07\"]):\n    ax.plot(p_grid[\"clf__n_estimators\"], scores[i], label='learning_rate: ' + str(value))\nax.legend()\nax.set_xlabel('n_estimators')\nax.set_ylabel('Mean Absolute Error')\nax.set_title(\"MAE across learning rates and n_estimators\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T21:50:15.930090Z","iopub.execute_input":"2022-12-17T21:50:15.930517Z","iopub.status.idle":"2022-12-17T21:52:08.602283Z","shell.execute_reply.started":"2022-12-17T21:50:15.930482Z","shell.execute_reply":"2022-12-17T21:52:08.601430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest\n\nNow for Random Forest. There are several more hyperparameters I could test out, however it is quite computationally expensive, even if I used Random Search. Thus I am searching a relatively small section of the hyperparameter space.\n\n\nnote: I tested out a wider range of n_estimators and different max_depths, however training took way too long, so I left them out for the final  version. No combinations acheived a significantly better MAE. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(criterion=\"absolute_error\",random_state=0)\n\np_grid = {\"clf__n_estimators\":[200,300,400,500,600,700,800,900],\n         \"clf__max_depth\":[10,None]}\n\ngs = get_hyper(model, p_grid)\n\nprint(\"Best MAE: %f using %s\" % (gs.best_score_, gs.best_params_))\n\nmeans = gs.cv_results_['mean_test_score']\nscores = np.array(means).reshape(len(p_grid[\"clf__max_depth\"]), len(p_grid[\"clf__n_estimators\"]))\n\nfig, ax = plt.subplots()\nfor i, value in enumerate(['10','None']):\n    ax.plot(p_grid[\"clf__n_estimators\"], scores[i], label=\"max_depth: \" + str(value))\nax.legend()\nax.set_xlabel(\"n_estimators\")\nax.set_ylabel(\"Mean Absolute Error\")\nax.set_title(\"MAE across n_estimators\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-17T21:52:28.967895Z","iopub.execute_input":"2022-12-17T21:52:28.968317Z","iopub.status.idle":"2022-12-17T22:01:55.349104Z","shell.execute_reply.started":"2022-12-17T21:52:28.968281Z","shell.execute_reply":"2022-12-17T22:01:55.348058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian Ridge\n\nThe hyperparamters for Bayesian ridge are the initial values of alpha and lambda. For this data, alpha and lambda always converge to the same values, regardless of the initial values, thus tuning is not necessary. For the sake of consistency, I test out several values of alpha_init, which all results in the same MAE.","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\n\nmodel = linear_model.BayesianRidge()\n\np_grid = {'clf__alpha_init':[1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.9],}\n\ngs = get_hyper(model, p_grid)\n\nprint(\"Best MAE: %f using %s\" % (gs.best_score_, gs.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-12-17T22:02:49.811468Z","iopub.execute_input":"2022-12-17T22:02:49.811882Z","iopub.status.idle":"2022-12-17T22:02:52.114721Z","shell.execute_reply.started":"2022-12-17T22:02:49.811850Z","shell.execute_reply":"2022-12-17T22:02:52.113337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nAll the the features influence exam scores in the direction you would expect. That is, factors that are a proxy for SES (parental education, standard/free lunch, test prep) all affect test performance. There are also disparities between racial groups, however they are not as significant. \n\nI successfully created a model that (decently) predicted performance. After tuning the hyperparameters of XGBoost and Random Forest, good 'ol linear regression beat both. Bayesian Ridge predicted student performance with an average MAE of 9.98, better than the other algorithms, however the difference is small.\n\nThis was mostly an exercise to practice tuning hyperparameters of different models, I am satisfied with the results. ","metadata":{}}]}